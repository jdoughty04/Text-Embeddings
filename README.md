# Text Embeddings
This repository contains a simple python notebook I used to experiment and learn with text embeddings. It uses a dataset of factoids (true and false statements) from https://huggingface.co/datasets/Dzeniks/FactFusion.

Text embeddings are generated by machine learning models trained to convert text into meaningful numerical representations. They convert strings of text into vectors in a high dimensional vector space in such a way that relative distance within the vector space reflects semantic similarity between the texts. With text being represented as high dimensional data points, we can use algorithms which give insight into the semantic structure of the data.

In this notebook, I apply dimensionality reduction techniques to visualize 2D representations of the embeddings, and also implement search functionality to find the most similar texts to a given query.



## Generating embeddings from text
In the get_embedding function, Distilbert-base-uncased (https://huggingface.co/distilbert/distilbert-base-uncased) is used to generate the high dimensional embeddings from text. This function also implements gpu acceleration if available.

## Dimensionality reduction
For the task of visualizing the embeddings, I compared the performace of PCA and UMAP. The plots are interactive, so mousing over a point will show the text. UMAP is the more complex method and seemed to produce a marginally better 2D representation, which can be seen by mousing over the points and seeing how similar facts are grouped together, especially when looking at the more compartmentalized clusters of points. Several UMAP visualizations are made using different values of the n_neighbors parameter.

## Search functionality
Cosine similarity is a method to quantify the similarity in direction between two vectors. In the search function, an embedding vector is generated from the input query. It then computes the similarity with all the embeddings in the dataset and returns the top k most similar texts.